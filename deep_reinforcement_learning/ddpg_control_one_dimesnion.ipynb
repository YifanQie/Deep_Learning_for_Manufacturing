{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook aims to develop Control and correction of Manufacturing Systems using Deep Reinforement Learning (CCMS-DRL)\n",
    "#Deep Q Learning\n",
    "#VRM Matlab Integration\n",
    "#Control and Correction\n",
    "#Markov Decsion Process\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inititating basic data structures\n",
    "param_headers=[]\n",
    "param_dim=3\n",
    "stage_dim=2\n",
    "dev_dim=3\n",
    "cop_dim=8047\n",
    "timesteps=1\n",
    "\n",
    "nominal_cop_x=np.zeros((timesteps, cop_dim))\n",
    "nominal_cop_y=np.zeros((timesteps, cop_dim))\n",
    "nominal_cop_z=np.zeros((timesteps, cop_dim))\n",
    "\n",
    "for i in range(param_dim):\n",
    "    param_headers.append(\"pp_\"+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting PCA for dimension Reduction\n",
    "\n",
    "dataset_x = pd.read_csv(\"./pca_data/pca_datacop_pca_drl_x_3_0.csv\",header=None).iloc[:, :-1]\n",
    "dataset_y = pd.read_csv(\"./pca_data/pca_datacop_pca_drl_y_3_0.csv\",header=None).iloc[:, :-1]\n",
    "dataset_z = pd.read_csv(\"./pca_data/pca_datacop_pca_drl_z_3_0.csv\",header=None).iloc[:, :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 24141)\n"
     ]
    }
   ],
   "source": [
    "# Preprocesing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "dev_data=pd.concat([dataset_x, dataset_y,dataset_z], axis = 1)\n",
    "print(dev_data.shape)\n",
    "scaler_t= MinMaxScaler()\n",
    "dev_data_t = scaler_t.fit_transform(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 8)\n",
      "[0.61500754 0.19004906 0.08689917 0.05935777 0.02187135 0.01608941\n",
      " 0.00658602 0.0032461 ]\n"
     ]
    }
   ],
   "source": [
    "#PCA with 95% variance explanantion\n",
    "var_limit=0.999\n",
    "from sklearn.decomposition import PCA\n",
    "pca_t = PCA(var_limit)\n",
    "#pca.fit(dev_data)\n",
    "dev_pc = pca_t.fit_transform(dev_data_t)\n",
    "print(dev_pc.shape)\n",
    "explained_var=pca_t.explained_variance_ratio_\n",
    "print(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.000000e+02</td>\n",
       "      <td>7.000000e+02</td>\n",
       "      <td>7.000000e+02</td>\n",
       "      <td>7.000000e+02</td>\n",
       "      <td>7.000000e+02</td>\n",
       "      <td>7.000000e+02</td>\n",
       "      <td>7.000000e+02</td>\n",
       "      <td>7.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.744586e-16</td>\n",
       "      <td>-6.090366e-16</td>\n",
       "      <td>-2.334640e-16</td>\n",
       "      <td>-1.624098e-16</td>\n",
       "      <td>-2.030122e-17</td>\n",
       "      <td>2.385393e-16</td>\n",
       "      <td>1.268826e-16</td>\n",
       "      <td>-1.763669e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.069757e+01</td>\n",
       "      <td>1.150567e+01</td>\n",
       "      <td>7.780131e+00</td>\n",
       "      <td>6.430102e+00</td>\n",
       "      <td>3.903165e+00</td>\n",
       "      <td>3.347720e+00</td>\n",
       "      <td>2.141858e+00</td>\n",
       "      <td>1.503695e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.353483e+01</td>\n",
       "      <td>-3.272252e+01</td>\n",
       "      <td>-2.114220e+01</td>\n",
       "      <td>-1.296156e+01</td>\n",
       "      <td>-9.469738e+00</td>\n",
       "      <td>-8.971204e+00</td>\n",
       "      <td>-6.882221e+00</td>\n",
       "      <td>-5.107640e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.491381e+01</td>\n",
       "      <td>-8.305674e+00</td>\n",
       "      <td>-5.322579e+00</td>\n",
       "      <td>-5.236408e+00</td>\n",
       "      <td>-2.935790e+00</td>\n",
       "      <td>-2.261339e+00</td>\n",
       "      <td>-1.264315e+00</td>\n",
       "      <td>-5.607888e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-8.835909e+00</td>\n",
       "      <td>3.367156e+00</td>\n",
       "      <td>2.220401e-01</td>\n",
       "      <td>-1.373003e-01</td>\n",
       "      <td>-3.723971e-01</td>\n",
       "      <td>-2.611092e-01</td>\n",
       "      <td>-5.137190e-02</td>\n",
       "      <td>8.929002e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.845730e+00</td>\n",
       "      <td>8.661230e+00</td>\n",
       "      <td>5.052208e+00</td>\n",
       "      <td>5.186665e+00</td>\n",
       "      <td>2.182402e+00</td>\n",
       "      <td>1.918606e+00</td>\n",
       "      <td>8.345282e-01</td>\n",
       "      <td>5.029736e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.114733e+01</td>\n",
       "      <td>4.422742e+01</td>\n",
       "      <td>2.127252e+01</td>\n",
       "      <td>1.376230e+01</td>\n",
       "      <td>1.103621e+01</td>\n",
       "      <td>1.630160e+01</td>\n",
       "      <td>2.107247e+01</td>\n",
       "      <td>9.158121e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  7.000000e+02  7.000000e+02  7.000000e+02  7.000000e+02  7.000000e+02   \n",
       "mean   9.744586e-16 -6.090366e-16 -2.334640e-16 -1.624098e-16 -2.030122e-17   \n",
       "std    2.069757e+01  1.150567e+01  7.780131e+00  6.430102e+00  3.903165e+00   \n",
       "min   -2.353483e+01 -3.272252e+01 -2.114220e+01 -1.296156e+01 -9.469738e+00   \n",
       "25%   -1.491381e+01 -8.305674e+00 -5.322579e+00 -5.236408e+00 -2.935790e+00   \n",
       "50%   -8.835909e+00  3.367156e+00  2.220401e-01 -1.373003e-01 -3.723971e-01   \n",
       "75%    7.845730e+00  8.661230e+00  5.052208e+00  5.186665e+00  2.182402e+00   \n",
       "max    8.114733e+01  4.422742e+01  2.127252e+01  1.376230e+01  1.103621e+01   \n",
       "\n",
       "                  5             6             7  \n",
       "count  7.000000e+02  7.000000e+02  7.000000e+02  \n",
       "mean   2.385393e-16  1.268826e-16 -1.763669e-16  \n",
       "std    3.347720e+00  2.141858e+00  1.503695e+00  \n",
       "min   -8.971204e+00 -6.882221e+00 -5.107640e+00  \n",
       "25%   -2.261339e+00 -1.264315e+00 -5.607888e-01  \n",
       "50%   -2.611092e-01 -5.137190e-02  8.929002e-02  \n",
       "75%    1.918606e+00  8.345282e-01  5.029736e-01  \n",
       "max    1.630160e+01  2.107247e+01  9.158121e+00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_pc\n",
    "dev_df = pd.DataFrame(dev_pc)\n",
    "dev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06083309, 0.14866408, 0.54730575, ..., 0.34938026, 0.3172509 ,\n",
       "        0.26667299],\n",
       "       [0.65156089, 0.3792449 , 0.65360388, ..., 0.50339834, 0.31380435,\n",
       "        0.45429322],\n",
       "       [0.89502996, 0.151917  , 0.81420031, ..., 0.3858864 , 0.24537683,\n",
       "        0.69688963],\n",
       "       ...,\n",
       "       [0.0683665 , 0.44938039, 0.45766938, ..., 0.31671602, 0.22242038,\n",
       "        0.42334898],\n",
       "       [0.2458102 , 0.62016534, 0.76323687, ..., 0.22439522, 0.25979887,\n",
       "        0.39387176],\n",
       "       [0.08721407, 0.46662595, 0.4135524 , ..., 0.33743256, 0.22574263,\n",
       "        0.43419451]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_p=MinMaxScaler()\n",
    "dev_pc_t = scaler_p.fit_transform(dev_pc)\n",
    "dev_pc_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.224822</td>\n",
       "      <td>0.425244</td>\n",
       "      <td>0.498464</td>\n",
       "      <td>0.485018</td>\n",
       "      <td>0.461804</td>\n",
       "      <td>0.354975</td>\n",
       "      <td>0.246192</td>\n",
       "      <td>0.358035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.197718</td>\n",
       "      <td>0.149522</td>\n",
       "      <td>0.183430</td>\n",
       "      <td>0.240613</td>\n",
       "      <td>0.190343</td>\n",
       "      <td>0.132463</td>\n",
       "      <td>0.076619</td>\n",
       "      <td>0.105406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082354</td>\n",
       "      <td>0.317308</td>\n",
       "      <td>0.372975</td>\n",
       "      <td>0.289073</td>\n",
       "      <td>0.318637</td>\n",
       "      <td>0.265497</td>\n",
       "      <td>0.200965</td>\n",
       "      <td>0.318725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.140415</td>\n",
       "      <td>0.469002</td>\n",
       "      <td>0.503699</td>\n",
       "      <td>0.479881</td>\n",
       "      <td>0.443644</td>\n",
       "      <td>0.344643</td>\n",
       "      <td>0.244354</td>\n",
       "      <td>0.364294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.299770</td>\n",
       "      <td>0.537801</td>\n",
       "      <td>0.617578</td>\n",
       "      <td>0.679102</td>\n",
       "      <td>0.568232</td>\n",
       "      <td>0.430890</td>\n",
       "      <td>0.276045</td>\n",
       "      <td>0.393292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3           4           5  \\\n",
       "count  700.000000  700.000000  700.000000  700.000000  700.000000  700.000000   \n",
       "mean     0.224822    0.425244    0.498464    0.485018    0.461804    0.354975   \n",
       "std      0.197718    0.149522    0.183430    0.240613    0.190343    0.132463   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.082354    0.317308    0.372975    0.289073    0.318637    0.265497   \n",
       "50%      0.140415    0.469002    0.503699    0.479881    0.443644    0.344643   \n",
       "75%      0.299770    0.537801    0.617578    0.679102    0.568232    0.430890   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "                6           7  \n",
       "count  700.000000  700.000000  \n",
       "mean     0.246192    0.358035  \n",
       "std      0.076619    0.105406  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.200965    0.318725  \n",
       "50%      0.244354    0.364294  \n",
       "75%      0.276045    0.393292  \n",
       "max      1.000000    1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_pc_t_df = pd.DataFrame(dev_pc_t)\n",
    "dev_pc_t_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building The dataset for training, inital Exploration\n",
    "# Enviornment Functions\n",
    "#<state_t,action_t, reward_t,state_t+1>\n",
    "time_steps=1\n",
    "point_dim=8047\n",
    "cop_state_dim=point_dim*3\n",
    "std_val=0\n",
    "ucl=4\n",
    "lcl=-4\n",
    "filename_base=\"./ddpg_data/state_t_\"\n",
    "ucl_vector=[ucl]*param_dim\n",
    "lcl_vector=[lcl]*param_dim\n",
    "file_name_cop=[\"./ddpg_data/cop_drl_x_\",\"./ddpg_data/cop_drl_y_\",\"./ddpg_data/cop_drl_z_\"]\n",
    "\n",
    "def get_initial_state(time_steps,run_id,initial_sample_size=1):\n",
    "    \n",
    "    \n",
    "    initial_samples=np.zeros((initial_sample_size, 3))\n",
    "    \n",
    "    for i in range(param_dim):\n",
    "        initial_samples[:,i]=np.random.uniform(lcl_vector[i],ucl_vector[i],initial_sample_size)\n",
    "        #initial_samples[:,i]=np.random.uniform(-0.5,0.5,initial_sample_size)\n",
    "        \n",
    "    file_path=filename_base+str(run_id)\n",
    "    \n",
    "    np.savetxt(file_path+\".csv\",initial_samples, delimiter=\",\")\n",
    "    \n",
    "    #Running Simulations\n",
    "    #print(initial_samples)\n",
    "    run_system_model(initial_samples[0,:],timesteps,run_id)\n",
    "    \n",
    "    cop_tensor,cop_state=get_cop_tensor(run_id)\n",
    "    \n",
    "    kcc_state=initial_samples\n",
    "    \n",
    "    return cop_state,kcc_state\n",
    "\n",
    "def run_simulations(run_id,type_flag=\"drl\"):\n",
    "    \n",
    "    import matlab.engine\n",
    "    \n",
    "    print(\"Initiating Matlab Engine...\")\n",
    "\n",
    "    #Initiating CAE engine within AI environment\n",
    "    eng = matlab.engine.start_matlab()\n",
    "\n",
    "    #change to absolute path here\n",
    "    #eng.cd(r'C:\\Users\\sinha_s\\Desktop\\VRM - GUI - datagen\\Demos',nargout=0)\n",
    "    #Chnaging to Cross Member Assembly\n",
    "    eng.cd(r'C:\\Users\\SINHA_S\\Desktop\\cross_member_datagen\\Demos\\Fixture simulation\\Multi station\\locator_halo',nargout=0)\n",
    "\n",
    "    print(\"Initiating CAE simulations for run ID: \",run_id)\n",
    "    #print(\"Runnning MatLab\")\n",
    "    \n",
    "    eng.halo_reinforcement_learning(run_id,type_flag,nargout=0)\n",
    "    \n",
    "    print(\"Simulation_Completed\")\n",
    "\n",
    "#state, reward, done, info = env.step(action)\n",
    "def run_system_model(action_kccs,timesteps,run_id):\n",
    "    \n",
    "    action_matrix=np.zeros((timesteps, param_dim))\n",
    "    std_vector_gen=[0.0,0.0,0.0]\n",
    "    \n",
    "    for i in range(param_dim):\n",
    "        action_matrix[:,i]=np.clip(np.random.normal(action_kccs[i], std_vector_gen[i], timesteps),lcl_vector[i],ucl_vector[i])\n",
    "    \n",
    "    file_path=filename_base+str(run_id)\n",
    "    \n",
    "    np.savetxt(file_path+\".csv\", action_matrix, delimiter=\",\")\n",
    "    \n",
    "    #Running Multi-Physcis Based VRM Model\n",
    "    run_simulations(run_id)\n",
    "    \n",
    "    #CALL OSER MODEL HERE\n",
    "    #state_matrix=run_OSER_model()\n",
    "    cop_tensor,cop_state=get_cop_tensor(run_id)\n",
    "    \n",
    "    #CURRENTLY STATE ESTIMATES are jittered based on OSER model accuracy\n",
    "    #state_matrix=action_matrix\n",
    "    \n",
    "    #state=np.mean(state_matrix, axis=0)\n",
    "    \n",
    "    return cop_state\n",
    "\n",
    "def get_cop_tensor(run_id):\n",
    "    \n",
    "    cop_tensor=[]\n",
    "    \n",
    "    for file in file_name_cop:\n",
    "        cop_tensor.append(pd.read_csv(file+str(run_id)+\".csv\",header=None).iloc[:, :-1])\n",
    "    \n",
    "    #cop_state=np.zeros((3, point_dim))\n",
    "    cop_state=pd.concat([cop_tensor[0],cop_tensor[1],cop_tensor[2]], axis = 1)\n",
    "    \n",
    "    #cop_index=0\n",
    "    #print(cop_state.shape)\n",
    "    #for cop in cop_tensor:\n",
    "        #cop_state[cop_index,:]=cop.values[0,:]\n",
    "        #print(cop.values[0,:].shape)\n",
    "        #print(cop.values)\n",
    "        #cop_index=cop_index+1\n",
    "    \n",
    "    #cop_state=cop_state.flatten()\n",
    "    \n",
    "    cop_scale_transform=scaler_t.transform(cop_state)\n",
    "    \n",
    "    #print(\"Cop State: \",cop_scale_transform)\n",
    "    \n",
    "    cop_state_transform=pca_t.transform(cop_scale_transform)\n",
    "    \n",
    "    cop_state_scale_transform=scaler_p.transform(cop_state_transform)\n",
    "   \n",
    "    cop_state_scale_transform=cop_state_scale_transform.flatten()\n",
    "    \n",
    "    print(\"Cop State: \",cop_state_scale_transform)\n",
    "    \n",
    "    return cop_tensor,cop_state_scale_transform\n",
    "\n",
    "def get_reward(cop_tensor,state_matrix,action_kccs):\n",
    "    \n",
    "        kcc_wts=np.array([0.3,0.3,0.3])\n",
    "        sys_rigid=0.5\n",
    "        current_kccs=np.mean(state_matrix, axis=0)\n",
    "        #KCC Loss\n",
    "        kcc_sse = np.absolute(current_kccs - action_kccs)\n",
    "        kcc_loss = np.sum(kcc_sse * kcc_wts) / np.sum(kcc_wts)\n",
    "        \n",
    "        #print(cop_tensor[0].shape,nominal_cop_x.shape)\n",
    "        #KPI Loss\n",
    "        kpi_loss = (np.absolute(cop_tensor[0] - nominal_cop_x)).values.mean()+(np.absolute(cop_tensor[1] - nominal_cop_x)).values.mean()+(np.absolute(cop_tensor[2] - nominal_cop_x)).values.mean()\n",
    "        \n",
    "        scaling_factor=0.1\n",
    "        kpi_loss=scaling_factor*kpi_loss\n",
    "        \n",
    "        #Sytem_Loss\n",
    "        system_loss=2*((1-sys_rigid)*kpi_loss+sys_rigid*kcc_loss)\n",
    "        \n",
    "        #Reward as negative of system loss\n",
    "        reward= -1*system_loss\n",
    "        \n",
    "        return reward, kpi_loss,kcc_loss\n",
    "\n",
    "def get_reward_signal(reward, loss_thres=-0.3,los_neg_thres=-2):\n",
    "    \n",
    "    #less_thres can be annleaded with time for continous improvement\n",
    "    reward_signal=0.0\n",
    "    \n",
    "    #Scaling Rewards Between 0 to 1\n",
    "    \n",
    "    if(reward>loss_thres):\n",
    "        reward_signal=1.0\n",
    "    elif(reward<los_neg_thres):\n",
    "        reward_signal=0.0\n",
    "    else:\n",
    "        reward_signal=(((reward-los_neg_thres)/(loss_thres-los_neg_thres))*1)\n",
    "    \n",
    "    return reward_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state, reward, done, info = env.step(action)\n",
    "\n",
    "def envior_step(action_kccs,state_matrix,run_id,kpi_thres=0.3):\n",
    "    cop_state=run_system_model(action_kccs,time_steps,run_id)\n",
    "    cop_tensor,cop_state=get_cop_tensor(run_id)\n",
    "    reward, kpi_loss,kcc_loss=get_reward(cop_tensor,state_matrix,action_kccs)\n",
    "    \n",
    "    reward_signal=get_reward_signal(reward, loss_thres=-0.4,los_neg_thres=-10)\n",
    "    info={\"Reward\": reward,\"Reward Signal\":reward_signal,\"KPI Loss\": kpi_loss,\"KCC Loss\": kcc_loss}\n",
    "    \n",
    "    if(kpi_loss<kpi_thres):\n",
    "        done=True\n",
    "    else:\n",
    "        done=False\n",
    "    \n",
    "    return cop_state,reward,done,info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manufacturing Enviornment System Test\n",
    "#Test Run: Succsess\n",
    "#run_id=5\n",
    "#cop_state,kcc_state=get_initial_state(time_steps,run_id)\n",
    "#action_kccs=kcc_state[0,:]\n",
    "\n",
    "#print(action_kccs)\n",
    "#envior_step(action_kccs,kcc_state,run_id,kpi_thres=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Vector of the System:  8\n",
      "Action vector of the System:  3\n"
     ]
    }
   ],
   "source": [
    "# Reinforcement Learning Model \n",
    "#num_states=param_dim*time_steps\n",
    "pca_output_dim=dev_pc.shape[1]\n",
    "num_states=pca_output_dim\n",
    "print(\"State Vector of the System: \",num_states)\n",
    "\n",
    "num_actions=3\n",
    "#param_dim=1\n",
    "print(\"Action vector of the System: \",num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ornstein-Uhlenbeck process for Exploration and Exploitation\n",
    "\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experience Relay Buffer Class\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float64)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "       \n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "def update_target(tau):\n",
    "    new_weights = []\n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights):\n",
    "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
    "\n",
    "    target_critic.set_weights(new_weights)\n",
    "\n",
    "    new_weights = []\n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights):\n",
    "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
    "\n",
    "    target_actor.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Actor and Critic neural networks\n",
    "from tensorflow.keras import layers\n",
    "#Setting upper bound based on upper control limit\n",
    "upper_bound=ucl\n",
    "lower_bound=lcl\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.0003, maxval=0.0003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "    state_out = layers.BatchNormalization()(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "    action_out = layers.BatchNormalization()(action_out)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1,kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def Policy as adding noise to the action\n",
    "# To be updated to noise depedent on the Bayesain Model Uncertianity\n",
    "\n",
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    print(\"Sampled Action: \",sampled_actions)\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "    print(\"Sampled Action after noise: \",sampled_actions,\" Noise: \",noise)\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    #import random  \n",
    "    #greedy_factor=random.uniform(0, 1)\n",
    "    \n",
    "    #greedy_threshold=0.2\n",
    "    #normal_noise_threshold=0.2\n",
    "    \n",
    "    #if(greedy_factor>greedy_threshold):\n",
    "        #sampled_actions = sampled_actions.numpy()\n",
    "    \n",
    "    #if(greedy_factor<greedy_threshold):\n",
    "        #print(\"Random Action Selected\")\n",
    "        #sampled_actions = np.array(np.array(random.uniform(-1, 1)))\n",
    "    \n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Training Hyper_parameters\n",
    "#std_dev = np.array([0.3,0.3,0.3]).astype(np.float64)\n",
    "std_dev=0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(num_actions), std_deviation=std_dev * np.ones(num_actions))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 200\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.3\n",
    "# Used to update target networks\n",
    "tau = 0.1\n",
    "\n",
    "buffer = Buffer(1000, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Shape Error Correction using Deep Reinforcement Learning ...\n",
      "Run ID for inital step:  0\n",
      "Initiating Matlab Engine...\n",
      "Initiating CAE simulations for run ID:  0\n",
      "Simulation_Completed\n",
      "Cop State:  [0.19713347 0.55804178 0.46510014 0.50982585 0.42249233 0.3412608\n",
      " 0.25371952 0.37814959]\n",
      "Cop State:  [0.19713347 0.55804178 0.46510014 0.50982585 0.42249233 0.3412608\n",
      " 0.25371952 0.37814959]\n",
      "[[ 2.22311814  2.58771803 -0.0608289 ]]\n",
      "Sampled Action:  tf.Tensor([0.00054947 0.00135151 0.00145899], shape=(3,), dtype=float64)\n",
      "Sampled Action after noise:  [-0.02279184 -0.01149278 -0.02362485]  Noise:  [-0.02334131 -0.01284429 -0.02508384]\n",
      "Action:  [array([-0.02279184, -0.01149278, -0.02362485])]\n",
      "[[ 2.22311814  2.58771803 -0.0608289 ]] [array([-0.02279184, -0.01149278, -0.02362485])]\n",
      "Initiating Matlab Engine...\n",
      "Initiating CAE simulations for run ID:  1\n",
      "Simulation_Completed\n",
      "Cop State:  [0.13691857 0.53078395 0.50896165 0.52264643 0.48465126 0.41016647\n",
      " 0.18583143 0.36007133]\n",
      "Cop State:  [0.13691857 0.53078395 0.50896165 0.52264643 0.48465126 0.41016647\n",
      " 0.18583143 0.36007133]\n",
      "Previous State:  [[ 2.22311814  2.58771803 -0.0608289 ]] New State:  [-0.02279184 -0.01149278 -0.02362485]\n",
      "{'Reward': -2.0605078978130265, 'Reward Signal': 0.8270304273111431, 'KPI Loss': 0.433066281879094, 'KCC Loss': 1.6274416159339324}\n",
      "Reward Signal:  -2.0605078978130265\n",
      "Overall Run ID:  1\n",
      "Episode Run ID:  0\n",
      "Within Episode Run ID:  0\n",
      "Previous KCC State:  [-0.02279184 -0.01149278 -0.02362485]\n",
      "Sampled Action:  tf.Tensor([-0.05021601  0.05352799  0.05350952], shape=(3,), dtype=float64)\n",
      "Sampled Action after noise:  [-0.08823927  0.00547266  0.05999447]  Noise:  [-0.03802326 -0.04805533  0.00648495]\n",
      "Action:  [array([-0.08823927,  0.00547266,  0.05999447])]\n",
      "[-0.02279184 -0.01149278 -0.02362485] [array([-0.08823927,  0.00547266,  0.05999447])]\n",
      "Initiating Matlab Engine...\n",
      "Initiating CAE simulations for run ID:  2\n",
      "Simulation_Completed\n",
      "Cop State:  [0.11951637 0.5257745  0.52313427 0.52605662 0.49609766 0.42672878\n",
      " 0.16733152 0.36215212]\n",
      "Cop State:  [0.11951637 0.5257745  0.52313427 0.52605662 0.49609766 0.42672878\n",
      " 0.16733152 0.36215212]\n",
      "Previous State:  [-0.02279184 -0.01149278 -0.02362485] New State:  [-0.08823927  0.00547266  0.05999447]\n",
      "{'Reward': -1.1097625952246286, 'Reward Signal': 0.9260663963307679, 'KPI Loss': 1.052092741181832, 'KCC Loss': 0.05766985404279677}\n",
      "Reward Signal:  -1.1097625952246286\n",
      "Overall Run ID:  2\n",
      "Episode Run ID:  0\n",
      "Within Episode Run ID:  1\n",
      "Previous KCC State:  [-0.08823927  0.00547266  0.05999447]\n",
      "Sampled Action:  tf.Tensor([-0.01361433  0.04301137  0.08881411], shape=(3,), dtype=float64)\n",
      "Sampled Action after noise:  [-0.03739546 -0.01496316  0.06420039]  Noise:  [-0.02378112 -0.05797453 -0.02461372]\n",
      "Action:  [array([-0.03739546, -0.01496316,  0.06420039])]\n",
      "[-0.08823927  0.00547266  0.05999447] [array([-0.03739546, -0.01496316,  0.06420039])]\n",
      "Initiating Matlab Engine...\n",
      "Initiating CAE simulations for run ID:  3\n"
     ]
    },
    {
     "ename": "MatlabExecutionError",
     "evalue": "\n  File C:\\Program Files\\MATLAB\\R2019b\\toolbox\\matlab\\iofun\\csvwrite.m, line 47, in csvwrite\n\n  File C:\\Users\\SINHA_S\\Desktop\\cross_member_datagen\\Demos\\Fixture simulation\\Multi station\\locator_halo\\halo_reinforcement_learning.m, line 243, in halo_reinforcement_learning\nCannot open file C:\\Users\\SINHA_S\\Desktop\\cross_member_datagen\\Demos\\Fixture simulation\\Multi station\\locator_halo\\ddpg_data\\cop_drl_x_3.csv.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMatlabExecutionError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c4be0f710692>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_state_kcc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menvior_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_state_kcc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkpi_thres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m#Rewards need to be normalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-0659d67bc6d2>\u001b[0m in \u001b[0;36menvior_step\u001b[1;34m(action_kccs, state_matrix, run_id, kpi_thres)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0menvior_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_kccs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkpi_thres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mcop_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_system_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_kccs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mcop_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcop_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_cop_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkpi_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkcc_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcop_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_kccs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-96c6ec9a8972>\u001b[0m in \u001b[0;36mrun_system_model\u001b[1;34m(action_kccs, timesteps, run_id)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;31m#Running Multi-Physcis Based VRM Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mrun_simulations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m#CALL OSER MODEL HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-96c6ec9a8972>\u001b[0m in \u001b[0;36mrun_simulations\u001b[1;34m(run_id, type_flag)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m#print(\"Runnning MatLab\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0meng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhalo_reinforcement_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtype_flag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnargout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Simulation_Completed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\matlab\\engine\\matlabengine.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             return FutureResult(self._engine(), future, nargs, _stdout,\n\u001b[1;32m---> 71\u001b[1;33m                                 _stderr, feval=True).result()\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__validate_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\matlab\\engine\\futureresult.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpythonengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TimeoutCannotBeNegative'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__future\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcancel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\matlab\\engine\\fevalfuture.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpythonengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MatlabFunctionTimeout'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpythonengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetFEvalResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_future\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nargout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_retrieved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMatlabExecutionError\u001b[0m: \n  File C:\\Program Files\\MATLAB\\R2019b\\toolbox\\matlab\\iofun\\csvwrite.m, line 47, in csvwrite\n\n  File C:\\Users\\SINHA_S\\Desktop\\cross_member_datagen\\Demos\\Fixture simulation\\Multi station\\locator_halo\\halo_reinforcement_learning.m, line 243, in halo_reinforcement_learning\nCannot open file C:\\Users\\SINHA_S\\Desktop\\cross_member_datagen\\Demos\\Fixture simulation\\Multi station\\locator_halo\\ddpg_data\\cop_drl_x_3.csv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Learning from VRM system\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "plt.ion() ## Note this correction\n",
    "fig=plt.figure()\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "avg_ep_reward_list=[]\n",
    "ep_run_length=[]\n",
    "\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "avg_episode_reward_list= []\n",
    "\n",
    "run_id=0\n",
    "episode_run_id=0\n",
    "\n",
    "\n",
    "print(\"Object Shape Error Correction using Deep Reinforcement Learning ...\")\n",
    "\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    #prev_state = env.reset()\n",
    "    #Getting inital state of the manufacturing system\n",
    "    \n",
    "    print(\"Run ID for inital step: \", run_id)\n",
    "    prev_state_cop,prev_state_kcc=get_initial_state(time_steps,run_id)\n",
    "    print(prev_state_kcc)\n",
    "    run_id=run_id+1\n",
    "    #prev_state=prev_state.flatten()\n",
    "    #prev_state=np.mean(prev_state, axis=0)\n",
    "    episodic_reward = 0\n",
    "    within_episode_run_id=0\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state_cop), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "        \n",
    "        print(\"Action: \",action)\n",
    "        # Recieve state and reward from environment.\n",
    "        action_input=np.zeros(3)\n",
    "        action_input=action[0]\n",
    "        \n",
    "        print(prev_state_kcc,action)\n",
    "        state, reward, done, info=envior_step(action_input,prev_state_kcc,run_id,kpi_thres=0.3)\n",
    "        \n",
    "        #Rewards need to be normalized\n",
    "        \n",
    "        print(\"Previous State: \", prev_state_kcc,\"New State: \",action_input)\n",
    "        \n",
    "        #state, reward, done, info = env.step(action)\n",
    "        \n",
    "        print(info)\n",
    "        print(\"Reward Signal: \", reward)\n",
    "        \n",
    "        buffer.record((prev_state_cop, action[0], reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(tau)\n",
    "\n",
    "        print(\"Overall Run ID: \",run_id)\n",
    "        print(\"Episode Run ID: \",episode_run_id)\n",
    "        print(\"Within Episode Run ID: \",within_episode_run_id)\n",
    "        \n",
    "        #increase Run_ID\n",
    "        run_id=run_id+1\n",
    "        within_episode_run_id=within_episode_run_id+1\n",
    "        \n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state_cop = state\n",
    "        \n",
    "        prev_state_kcc=action_input\n",
    "        \n",
    "        print(\"Previous KCC State: \",prev_state_kcc)\n",
    "        \n",
    "    #Appending Within List\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    avg_ep_reward_list.append(episodic_reward/within_episode_run_id)\n",
    "    ep_run_length.append(within_episode_run_id+1)\n",
    "                              \n",
    "    print(\"Episodic Reward: \",episodic_reward)\n",
    "    print(\"Average Episodic Reward: \",episodic_reward/within_episode_run_id,\" No of Runs in Episode: \",within_episode_run_id)\n",
    "    \n",
    "    #increase Episode Run_ID\n",
    "    episode_run_id=episode_run_id+1\n",
    "    \n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    avg_reward_episode = np.mean(avg_ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {} Episodic AVg Reward ==> {}\".format(ep, avg_reward,avg_reward_episode))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "    avg_episode_reward_list.append(avg_reward_episode)\n",
    "    \n",
    "    x.append(episode_run_id)\n",
    "    y.append(episodic_reward)\n",
    "    plt.scatter(x,y)\n",
    "    plt.show()\n",
    "    plt.pause(0.05) #Note this correction\n",
    "    #clear_output(wait=True)\n",
    "                              \n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
