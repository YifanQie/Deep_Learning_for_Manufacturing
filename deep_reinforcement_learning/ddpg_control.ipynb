{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook aims to develop Control and correction of Manufacturing Systems using Deep Reinforement Learning (CCMS-DRL)\n",
    "#Deep Q Learning\n",
    "#VRM Matlab Integration\n",
    "#Control and Correction\n",
    "#Markov Decsion Process\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inititating basic data structures\n",
    "param_headers=[]\n",
    "param_dim=3\n",
    "stage_dim=2\n",
    "dev_dim=3\n",
    "cop_dim=8047\n",
    "timesteps=1\n",
    "\n",
    "nominal_cop_x=np.zeros((timesteps, cop_dim))\n",
    "nominal_cop_y=np.zeros((timesteps, cop_dim))\n",
    "nominal_cop_z=np.zeros((timesteps, cop_dim))\n",
    "\n",
    "for i in range(param_dim):\n",
    "    param_headers.append(\"pp_\"+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting PCA for dimension Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building The dataset for training, inital Exploration\n",
    "# Enviornment Functions\n",
    "#<state_t,action_t, reward_t,state_t+1>\n",
    "time_steps=1\n",
    "point_dim=8047\n",
    "cop_state_dim=point_dim*3\n",
    "std_val=0\n",
    "ucl=1\n",
    "lcl=-1\n",
    "filename_base=\"state_t_\"\n",
    "ucl_vector=[ucl]*param_dim\n",
    "lcl_vector=[lcl]*param_dim\n",
    "file_name_cop=[\"cop_drl_x_\",\"cop_drl_y_\",\"cop_drl_z_\"]\n",
    "\n",
    "def get_initial_state(time_steps,run_id,initial_sample_size=1):\n",
    "    \n",
    "    initial_samples=np.zeros((initial_sample_size, param_dim))\n",
    "    \n",
    "    for i in range(param_dim):\n",
    "        #initial_samples[:,i]=np.random.uniform(lcl_vector[i],ucl_vector[i],initial_sample_size)\n",
    "        initial_samples[:,i]=np.random.uniform(-0.3,0.3,initial_sample_size)\n",
    "        \n",
    "    file_path=filename_base+str(run_id)\n",
    "    \n",
    "    np.savetxt(file_path+\".csv\",initial_samples, delimiter=\",\")\n",
    "    \n",
    "    #Running Simulations\n",
    "    #print(initial_samples)\n",
    "    run_system_model(initial_samples[0,:],timesteps,run_id)\n",
    "    \n",
    "    cop_tensor,cop_state=get_cop_tensor(run_id)\n",
    "    \n",
    "    kcc_state=initial_samples\n",
    "    \n",
    "    return cop_state,kcc_state\n",
    "\n",
    "def run_simulations(run_id,type_flag=\"drl\"):\n",
    "    \n",
    "    import matlab.engine\n",
    "    \n",
    "    print(\"Initiating Matlab Engine...\")\n",
    "\n",
    "    #Initiating CAE engine within AI environment\n",
    "    eng = matlab.engine.start_matlab()\n",
    "\n",
    "    #change to absolute path here\n",
    "    #eng.cd(r'C:\\Users\\sinha_s\\Desktop\\VRM - GUI - datagen\\Demos',nargout=0)\n",
    "    #Chnaging to Cross Member Assembly\n",
    "    eng.cd(r'C:\\Users\\sinha_s\\Desktop\\cross_member_datagen\\Demos\\Fixture simulation\\Multi station\\[1] Locator placement +Clamp (Door halo)',nargout=0)\n",
    "\n",
    "    print(\"Initiating CAE simulations for run ID: \",run_id)\n",
    "    #print(\"Runnning MatLab\")\n",
    "    \n",
    "    eng.halo_reinforcement_learning(run_id,type_flag,nargout=0)\n",
    "    \n",
    "    print(\"Simulation_Completed\")\n",
    "\n",
    "#state, reward, done, info = env.step(action)\n",
    "def run_system_model(action_kccs,timesteps,run_id):\n",
    "    \n",
    "    action_matrix=np.zeros((timesteps, param_dim))\n",
    "    std_vector_gen=[0.0,0.0,0.0]\n",
    "    \n",
    "    for i in range(param_dim):\n",
    "        action_matrix[:,i]=np.clip(np.random.normal(action_kccs[i], std_vector_gen[i], timesteps),lcl_vector[i],ucl_vector[i])\n",
    "    \n",
    "    file_path=filename_base+str(run_id)\n",
    "    \n",
    "    np.savetxt(file_path+\".csv\", action_matrix, delimiter=\",\")\n",
    "    \n",
    "    #Running Multi-Physcis Based VRM Model\n",
    "    run_simulations(run_id)\n",
    "    \n",
    "    #CALL OSER MODEL HERE\n",
    "    #state_matrix=run_OSER_model()\n",
    "    cop_tensor,cop_state=get_cop_tensor(run_id)\n",
    "    \n",
    "    #CURRENTLY STATE ESTIMATES are jittered based on OSER model accuracy\n",
    "    #state_matrix=action_matrix\n",
    "    \n",
    "    #state=np.mean(state_matrix, axis=0)\n",
    "    \n",
    "    return cop_state\n",
    "\n",
    "def get_cop_tensor(run_id):\n",
    "    \n",
    "    cop_tensor=[]\n",
    "    \n",
    "    for file in file_name_cop:\n",
    "        cop_tensor.append(pd.read_csv(file+str(run_id)+\".csv\",header=None).iloc[:,0:point_dim])\n",
    "    \n",
    "    cop_state=np.zeros((3, point_dim))\n",
    "    \n",
    "    cop_index=0\n",
    "    #print(cop_state.shape)\n",
    "    for cop in cop_tensor:\n",
    "        cop_state[cop_index,:]=cop.values[0,:]\n",
    "        #print(cop.values[0,:].shape)\n",
    "        #print(cop.values)\n",
    "        cop_index=cop_index+1\n",
    "    \n",
    "    cop_state=cop_state.flatten()\n",
    "    \n",
    "    #print(\"Cop State: \",cop_state.shape)\n",
    "    return cop_tensor,cop_state\n",
    "\n",
    "def get_reward(cop_tensor,state_matrix,action_kccs):\n",
    "    \n",
    "        kcc_wts=np.array([0.3,0.3,0.3])\n",
    "        sys_rigid=0.5\n",
    "        current_kccs=np.mean(state_matrix, axis=0)\n",
    "        #KCC Loss\n",
    "        kcc_sse = np.absolute(current_kccs - action_kccs)\n",
    "        kcc_loss = np.sum(kcc_sse * kcc_wts) / np.sum(kcc_wts)\n",
    "        \n",
    "        #print(cop_tensor[0].shape,nominal_cop_x.shape)\n",
    "        #KPI Loss\n",
    "        kpi_loss = (np.absolute(cop_tensor[0] - nominal_cop_x)).values.mean()+(np.absolute(cop_tensor[1] - nominal_cop_x)).values.mean()+(np.absolute(cop_tensor[2] - nominal_cop_x)).values.mean()\n",
    "        \n",
    "        scaling_factor=0.1\n",
    "        kpi_loss=scaling_factor*kpi_loss\n",
    "        \n",
    "        #Sytem_Loss\n",
    "        system_loss=2*((1-sys_rigid)*kpi_loss+sys_rigid*kcc_loss)\n",
    "        \n",
    "        #Reward as negative of system loss\n",
    "        reward= -1*system_loss\n",
    "        \n",
    "        return reward, kpi_loss,kcc_loss\n",
    "\n",
    "def get_reward_signal(reward, loss_thres=-0.4,los_neg_thres=-3):\n",
    "    \n",
    "    #less_thres can be annleaded with time for continous improvement\n",
    "    reward_signal=0.0\n",
    "    \n",
    "    #Scaling Rewards Between 0 to 1\n",
    "    \n",
    "    if(reward>loss_thres):\n",
    "        reward_signal=1.0\n",
    "    elif(reward<los_neg_thres):\n",
    "        reward_signal=0.0\n",
    "    else:\n",
    "        reward_signal=(((reward-los_neg_thres)/(loss_thres-los_neg_thres))*1)\n",
    "    \n",
    "    return reward_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state, reward, done, info = env.step(action)\n",
    "\n",
    "def envior_step(action_kccs,state_matrix,run_id,kpi_thres=0.5):\n",
    "    cop_state=run_system_model(action_kccs,time_steps,run_id)\n",
    "    cop_tensor,cop_state=get_cop_tensor(run_id)\n",
    "    reward, kpi_loss,kcc_loss=get_reward(cop_tensor,state_matrix,action_kccs)\n",
    "    \n",
    "    reward_signal=get_reward_signal(reward, loss_thres=-0.4,los_neg_thres=-10)\n",
    "    info={\"Reward\": reward,\"Reward Signal\":reward_signal,\"KPI Loss\": kpi_loss,\"KCC Loss\": kcc_loss}\n",
    "    \n",
    "    if(kpi_loss<kpi_thres):\n",
    "        done=True\n",
    "    else:\n",
    "        done=False\n",
    "    \n",
    "    return cop_state,reward_signal,done,info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manufacturing Enviornment System Test\n",
    "#Test Run: Succsess\n",
    "#run_id=28\n",
    "#cop_state,kcc_state=get_initial_state(time_steps,run_id)\n",
    "#action_kccs=kcc_state[0,:]\n",
    "\n",
    "#print(action_kccs)\n",
    "#envior_step(action_kccs,kcc_state,run_id,kpi_thres=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement Learning Model \n",
    "#num_states=param_dim*time_steps\n",
    "num_states=cop_state_dim\n",
    "print(\"State Vector of the System: \",num_states)\n",
    "\n",
    "num_actions=param_dim\n",
    "print(\"Action vector of the System: \",num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ornstein-Uhlenbeck process for Exploration and Exploitation\n",
    "\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experience Relay Buffer Class\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float64)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "       \n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "def update_target(tau):\n",
    "    new_weights = []\n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights):\n",
    "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
    "\n",
    "    target_critic.set_weights(new_weights)\n",
    "\n",
    "    new_weights = []\n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights):\n",
    "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
    "\n",
    "    target_actor.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Actor and Critic neural networks\n",
    "from tensorflow.keras import layers\n",
    "#Setting upper bound based on upper control limit\n",
    "upper_bound=ucl\n",
    "lower_bound=lcl\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(param_dim, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "    state_out = layers.BatchNormalization()(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(param_dim))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "    action_out = layers.BatchNormalization()(action_out)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1,kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def Policy as adding noise to the action\n",
    "# To be updated to noise depedent on the Bayesain Model Uncertianity\n",
    "\n",
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    #print(\"Sampled Action: \",sampled_actions)\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "    #print(\"Sampled Action after noise: \",sampled_actions,\" Noise: \",noise)\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Training Hyper_parameters\n",
    "std_dev = np.array([0.3,0.3,0.3]).astype(np.float64)\n",
    "ou_noise = OUActionNoise(mean=np.zeros(num_actions), std_deviation=std_dev * np.ones(num_actions))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning from VRM system\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "plt.ion() ## Note this correction\n",
    "fig=plt.figure()\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "avg_ep_reward_list=[]\n",
    "ep_run_length=[]\n",
    "\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "avg_episode_reward_list= []\n",
    "\n",
    "run_id=0\n",
    "episode_run_id=0\n",
    "\n",
    "\n",
    "print(\"Object Shape Error Correction using Deep Reinforcement Learning ...\")\n",
    "\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    #prev_state = env.reset()\n",
    "    #Getting inital state of the manufacturing system\n",
    "    \n",
    "    print(\"Run ID for inital step: \", run_id)\n",
    "    prev_state_cop,prev_state_kcc=get_initial_state(time_steps,run_id)\n",
    "    run_id=run_id+1\n",
    "    #prev_state=prev_state.flatten()\n",
    "    #prev_state=np.mean(prev_state, axis=0)\n",
    "    episodic_reward = 0\n",
    "    within_episode_run_id=0\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state_cop), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "        \n",
    "        print(\"Action: \",action)\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, info=envior_step(action[0],prev_state_kcc,run_id,kpi_thres=0.5)\n",
    "        \n",
    "        #Rewards need to be normalized\n",
    "        \n",
    "        print(\"Previous State: \", prev_state_kcc,\"New State: \",action[0])\n",
    "        \n",
    "        #state, reward, done, info = env.step(action)\n",
    "        \n",
    "        print(info)\n",
    "        print(\"Reward Signal: \", reward)\n",
    "        \n",
    "        buffer.record((prev_state_cop, action[0], reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(tau)\n",
    "\n",
    "        print(\"Overall Run ID: \",run_id)\n",
    "        print(\"Episode Run ID: \",episode_run_id)\n",
    "        print(\"Within Episode Run ID: \",within_episode_run_id)\n",
    "        \n",
    "        #increase Run_ID\n",
    "        run_id=run_id+1\n",
    "        within_episode_run_id=within_episode_run_id+1\n",
    "        \n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state_cop = state\n",
    "        prev_state_kcc=action[0]\n",
    "        \n",
    "    #Appending Within List\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    avg_ep_reward_list.append(episodic_reward/within_episode_run_id)\n",
    "    ep_run_length.append(within_episode_run_id+1)\n",
    "                              \n",
    "    print(\"Episodic Reward: \",episodic_reward)\n",
    "    print(\"Average Episodic Reward: \",episodic_reward/within_episode_run_id,\" No of Runs in Episode: \",within_episode_run_id)\n",
    "    \n",
    "    #increase Episode Run_ID\n",
    "    episode_run_id=episode_run_id+1\n",
    "    \n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    avg_reward_episode = np.mean(avg_ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {} Episodic AVg Reward ==> {}\".format(ep, avg_reward,avg_reward_episode))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "    avg_episode_reward_list.append(avg_reward_episode)\n",
    "    \n",
    "    x.append(episode_run_id)\n",
    "    y.append(episodic_reward)\n",
    "    plt.scatter(x,y)\n",
    "    plt.show()\n",
    "    plt.pause(0.05) #Note this correction\n",
    "    #clear_output(wait=True)\n",
    "                              \n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
